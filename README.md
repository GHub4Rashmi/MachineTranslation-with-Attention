# MachineTranslation-with-Attention
Machine Translation with Attention mechanism. An improvisation over normal seq2seq model for machine translation.
The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements.
It is a mechanism that is developed to increase the performance of encoder decoder(seq2seq) RNN model.
